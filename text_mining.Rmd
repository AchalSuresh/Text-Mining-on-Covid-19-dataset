---
title: "Text Mining"
output:
  word_document: default
  html_notebook: default
---

This text mining exercise is derived in part from "Tidy Text Mining with R" (https://www.tidytextmining.com/index.html).
Let's install the necessary packages. 
```{r}
# install.packages("wordcloud")  
# install.packages("twitteR")    
# install.packages("textdata")
# install.packages("SnowballC")
# install.packages("tm")
# install.packages("tidytext")


rm(list = ls())     # clear the workspace 
library(wordcloud) # creating a word cloud
library(twitteR) # package for connecting to Twitter
library(SnowballC)
library(tm)
library(tidytext)
library(tidyverse)

```

# Donwload tweets using API
To get your consumerKey and consumerSecret. First you need to create a developer account. Visit https://apps.twitter.com/ 

```{r, eval=F, echo=T}
consumer_key <-'consumer_key'
consumer_secret <-'consumer_secret'
access_token <-'access_token'
access_secret <-'access_secret'

setup_twitter_oauth(consumer_key,       # set up a connection with Twitter
                    consumer_secret,
                    access_token,
                    access_secret)
```

Get 10000 tweets that contain "#COVID19 "
```{r, eval=F, echo=T}
tweet_full<- searchTwitter("#COVID19", n=10000,lang="en") 
#save(tweet_full,file="tweet_full.RData")
```

If you couldn't (or don't want to) create a developer account and get tweets from Twitter, use the one I've downloaded. Please download tweet_full.RData and save it to your working directory. This command will load tweet data as `tweet_full` in your working environment. You can skip this part if you have downloaded your own data.

```{r}
load("tweet_full.RData")
```

# Preprocessing text data
## remove retweets
Let's remove retweets from the analysis. 
```{r}
# remove RT
tweet_no_rt<-strip_retweets(tweet_full)
```

## save tweets in a data table
The tweets are saved in a list form Let's transform the data format into a data table, where each row indicates each tweet  (i.e. one-row-per-document). Let's assign `ID` for each tweet.  
```{r}
tweet_text <- sapply(tweet_no_rt, function(x) x$getText())
tweet_tidy<-enframe(tweet_text)
tweet_tidy<-tweet_tidy%>%
  rename(body=value)%>%
  rowid_to_column("ID")

```
```{r}
#View the file

tweet_tidy
```

## tokenization
This converts the data table to a tidy text data (i.e. one-row-per-token) frame and automatically converts all tokens to lowercase.
```{r}
tweet_word<-tweet_tidy %>%
            unnest_tokens(word, body)
tweet_word
```

## remove stopwords and numbers
This removes numbers and common (as well as customized) stop words that are uninformative. 
```{r}
stop_words

custom_stop_words <- tibble(word = c("https","t.co","amp","covid19"))

tweet_word <- tweet_word %>%
            anti_join(stop_words, by=c("word"="word"))%>%                 # remove stopwords
            anti_join(custom_stop_words, by = c("word" = "word"))%>%      # remove custom stopwords
            filter(!str_detect(word, "^[0-9]*$")) %>%                     # remove number
            #mutate(word = wordStem(word)) %>%     # stem all the tokens to their root word
            mutate(word = str_replace_all(word, "https://t.co/[A-Za-z\\d]+|&amp;", ""))

tweet_word

```

# Frequency analysis
## top ten terms 

```{r}
tweet_word %>%
  count(word, sort = TRUE) %>%      # summarize count per word 
  top_n(20) %>%                     # retain top 20 frequent words
  mutate(word = reorder(word, n)) %>%  # sort based on the word count
  ggplot(aes(word, n)) +
  geom_col() +                      # create a bar chart with value n
  xlab(NULL) +
  coord_flip()
```
## create wordcloud
```{r}
tweet_word %>%
  group_by(word) %>%
  count() %>%
  with(wordcloud(word, n, max.words = 30))
```

#  Sentiment analysis
The basic idea of sentiment analysis that, when we as human readers understand a text, we use our general understanding of the sentimental intent of words in a document to infer the sentiment of the document. Thus, given the premise that a document is considered as a combination of its individual words, the sentiment of the whole text can be inferred as the sum of the sentiment of the individual words. 

## Sentiment lexicons
For the sentiment of English words, there exist several general-purpose, publicly-available lexicons, which are constructed via crowdsourcing or by the labor of the authors. While this lexicon-based method has limitations (e.g., loss of information from disregarding sentence structure), it is widely used for its simplicity and shows sufficiently good performance. 

BING lexicon assigns English words with negative and positive. Another popular lexicon, AFINN, assigns English words with an integer score between minus five (negative sentiment) and five (positive sentiment). To use AFINN lexicon, you need to install package `textdata`. 

```{r}
get_sentiments("bing")
get_sentiments("afinn")
```


## Sentiment analysis using inner_join

```{r}
tweet_bing <- tweet_word %>%
  inner_join(get_sentiments("bing"), by = c(word = "word"))

tweet_bing %>%
  count(word, sentiment, sort = TRUE)
```

```{r}
tweet_afinn<-tweet_word%>%
  inner_join(get_sentiments("afinn"),by=c(word="word"))

tweet_afinn %>%
  count(word, value, sort = TRUE) 
```

## contribution to sentiment

```{r}
tweet_bing %>%
  count(sentiment, word) %>%
  filter(n >= 10) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  ylab("Contribution to sentiment")+
  coord_flip()
```

## positive vs. negative documents
We may classify tweets with more positive terms as positive, while the ones with more negative terms as negative. 

```{r}
tweet_bing%>%
  group_by(ID)%>%
  summarise(positive=sum(sentiment=="positive"),negative=sum(sentiment=="negative"))%>%
  mutate(sentiment=positive-negative)

tweet_tidy<-tweet_bing%>%
  group_by(ID)%>%
  summarise(positive=sum(sentiment=="positive"),negative=sum(sentiment=="negative"))%>%
  mutate(sentiment=positive-negative)%>%
  right_join(tweet_tidy)

tweet_tidy
```

```{r}
tweet_tidy<-tweet_tidy%>% 
  mutate(positive_tweet=sentiment>0)

tweet_tidy%>%
  count(positive_tweet)

tweet_tidy%>%
  filter(!is.na(sentiment))%>%
  ggplot(aes(sentiment))+geom_histogram(binwidth=1)
```

## positive vs. non-positive reviews - frequent terms and bigrams
Compare frequent terms in positive and negative tweets. 
```{r}
tweet_bing %>%
  left_join(tweet_tidy%>%select(ID,positive_tweet))%>%
  group_by(positive_tweet) %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  filter(!is.na(positive_tweet))%>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word,n, fill=positive_tweet)) +
  geom_col(show.legend=FALSE)+
  facet_wrap(~ positive_tweet,ncol=2, scales = "free_y") +
  labs(y="Freqneucy of words",x=NULL)+
  coord_flip()
```


# Bi-gram analysis 
Bigram analysis considers adjacent pairs of words in documents to infer the context. 

## Count of bi-gram
We follow a similar procedure we did with single terms - tokening documents into 2-grams (bigrams), conducting necessary text cleaning processes (e.g., removing punctuation, numbers, and articles), and counting the frequency of bigrams. 

```{r}
tweet_bigrams <- tweet_tidy %>%
  unnest_tokens(bigram, body,token="ngrams",n=2)
  
tweet_bigrams
```

```{r}
tweet_bigrams %>%
  count(bigram, sort = TRUE)
```

```{r}
bigrams_separated <- tweet_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_separated
```

```{r}
# text processing
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% custom_stop_words$word)%>%
  filter(!word2 %in% custom_stop_words$word)%>%
  filter(!str_detect(word1, "^[0-9]*$")) %>%
  filter(!str_detect(word2, "^[0-9]*$")) %>%
  mutate(word1 = str_replace_all(word1, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  mutate(word2 = str_replace_all(word2, "https://t.co/[A-Za-z\\d]+|&amp;", ""))

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

```{r}
bigram_counts %>%
  unite(bigram,word1, word2, sep = " ")

bigram_counts %>%
  unite(bigram,word1, word2, sep = " ")%>%
  top_n(15) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x=bigram, y=n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

```{r}
install.packages("ggraph")

library(igraph)
library(ggraph)

bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
# Document-text matrix
Tidy text data frames are one-row-per-token, but to use some text mining functions from `tm` packages, we need our data in a one-row-per-document format. That is, a document-term matrix. We can `use cast_dtm()` to create a document-term matrix.

```{r}
tweet_dtm <- tweet_word %>%
  count(ID,word, sort = TRUE) %>%   ## get count of each token in each document
  cast_dtm(ID,word, n)    ## create a document-term matrix with all features and tf weighting

tweet_dtm
```

```{r}
# inspect corpus
dim(tweet_dtm)                         # check the dimension of the table 
inspect(tweet_dtm)
```

# Sparcity
We may want to reduce the number of text features (i.e. number if tokens (terms) in DTM) by removing sparse terms, which do not appear across many documents. 

For this, we can use `removeSparseTerms()` function. The first argument is a document-term matrix, and the second argument defines the maximal allowed sparsity in the range from 0 to 1. So for instance, sparse = .99 would remove any tokens which are missing from more than 99% of the documents in the corpus (i.e. the token must appear in at least 1% of the documents to be retained). 
```{r}
removeSparseTerms(tweet_dtm, sparse = .99)

tweet_dtm<-removeSparseTerms(tweet_dtm, sparse = .99)
```

Once you remove sparse terms, some documents may not contain any terms in DTM. Let's remove these documents. 
```{r}
rowTotals <- apply(tweet_dtm , 1, sum)             #Find the sum of words in each Document
tweet_dtm   <- tweet_dtm[rowTotals> 0, ]           #remove all docs without words

tweet_dtm
```
